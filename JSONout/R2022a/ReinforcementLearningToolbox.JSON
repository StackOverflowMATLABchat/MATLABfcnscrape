[
	"accelerate",
	"append",
	"barrierPenalty",
	"bus2RLSpec",
	"cleanup",
	"createGridWorld",
	"createIntegratedEnv",
	"createMDP",
	"evaluate",
	"exteriorPenalty",
	"generatePolicyFunction",
	"generateRewardFunction",
	"getAction",
	"getActionInfo",
	"getActor",
	"getCritic",
	"getLearnableParameters",
	"getMaxQValue",
	"getModel",
	"getObservationInfo",
	"getValue",
	"gradient",
	"hyperbolicPenalty",
	"inspectTrainingResult",
	"predict",
	"quadraticLayer",
	"reset",
	"rlACAgent",
	"rlACAgentOptions",
	"rlAgentInitializationOptions",
	"rlContinuousDeterministicActor",
	"rlContinuousDeterministicRewardFunction",
	"rlContinuousDeterministicTransitionFunction",
	"rlContinuousGaussianActor",
	"rlContinuousGaussianRewardFunction",
	"rlContinuousGaussianTransitionFunction",
	"rlCreateEnvTemplate",
	"rlDDPGAgent",
	"rlDDPGAgentOptions",
	"rlDeterministicActorRepresentation",
	"rlDiscreteCategoricalActor",
	"rlDQNAgent",
	"rlDQNAgentOptions",
	"rlFiniteSetSpec",
	"rlFunctionEnv",
	"rlIsDoneFunction",
	"rlMBPOAgent",
	"rlMBPOAgentOptions",
	"rlMDPEnv",
	"rlMultiAgentTrainingOptions",
	"rlNeuralNetworkEnvironment",
	"rlNumericSpec",
	"rlOptimizerOptions",
	"rlPGAgent",
	"rlPGAgentOptions",
	"rlPPOAgent",
	"rlPPOAgentOptions",
	"rlPredefinedEnv",
	"rlQAgent",
	"rlQAgentOptions",
	"rlQValueFunction",
	"rlQValueRepresentation",
	"rlReplayMemory",
	"rlRepresentation",
	"rlRepresentationOptions",
	"rlSACAgent",
	"rlSACAgentOptions",
	"rlSARSAAgent",
	"rlSARSAAgentOptions",
	"rlSimulationOptions",
	"rlSimulinkEnv",
	"rlStochasticActorRepresentation",
	"rlTable",
	"rlTD3Agent",
	"rlTD3AgentOptions",
	"rlTrainingOptions",
	"rlTRPOAgent",
	"rlTRPOAgentOptions",
	"rlValueFunction",
	"rlValueRepresentation",
	"rlVectorQValueFunction",
	"runEpisode",
	"sample",
	"scalingLayer",
	"setActor",
	"setCritic",
	"setLearnableParameters",
	"setModel",
	"setup",
	"sim",
	"SimulinkEnvWithAgent",
	"softplusLayer",
	"train",
	"validateEnvironment"
]
